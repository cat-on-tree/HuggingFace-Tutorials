{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGw4tSbKtPFdxPEMUHBnT6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cat-on-tree/HuggingFace-Tutorials/blob/main/HuggingfaceTutorial_1_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "px44-T7hXgxn"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#加载预训练语料和分词器\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    pretrained_model_name_or_path='bert-base-chinese',\n",
        "    cache_dir=None,\n",
        "    force_download=False,\n",
        ")\n",
        "\n",
        "sents = [\n",
        "    '选择珠江花园的原因就是方便。',\n",
        "    '笔记本的键盘确实爽。',\n",
        "    '房间太小。其他的都一般。',\n",
        "    '今天才知道这书还有第6卷,真有点郁闷.',\n",
        "    '机器背面似乎被撕了张什么标签，残胶还在。',\n",
        "]\n",
        "\n",
        "tokenizer, sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5dK4wXIX2Fv",
        "outputId": "f7535594-1b7b-49d4-806c-3b3e3848aadf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
              " \t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " \t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              " }\n",
              " ),\n",
              " ['选择珠江花园的原因就是方便。',\n",
              "  '笔记本的键盘确实爽。',\n",
              "  '房间太小。其他的都一般。',\n",
              "  '今天才知道这书还有第6卷,真有点郁闷.',\n",
              "  '机器背面似乎被撕了张什么标签，残胶还在。'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#示例：分词处理\n",
        "tokenized_sents = [tokenizer.tokenize(sent) for sent in sents]\n",
        "\n",
        "for sent, tokenized_sent in zip(sents, tokenized_sents):\n",
        " print(f\"原句: {sent}\")\n",
        " print(f\"分词: {tokenized_sent}\")\n",
        " print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGqaPX5TZs7m",
        "outputId": "67da6c6a-fd62-4343-83a0-47e96948beca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "原句: 选择珠江花园的原因就是方便。\n",
            "分词: ['选', '择', '珠', '江', '花', '园', '的', '原', '因', '就', '是', '方', '便', '。']\n",
            "\n",
            "原句: 笔记本的键盘确实爽。\n",
            "分词: ['笔', '记', '本', '的', '键', '盘', '确', '实', '爽', '。']\n",
            "\n",
            "原句: 房间太小。其他的都一般。\n",
            "分词: ['房', '间', '太', '小', '。', '其', '他', '的', '都', '一', '般', '。']\n",
            "\n",
            "原句: 今天才知道这书还有第6卷,真有点郁闷.\n",
            "分词: ['今', '天', '才', '知', '道', '这', '书', '还', '有', '第', '6', '卷', ',', '真', '有', '点', '郁', '闷', '.']\n",
            "\n",
            "原句: 机器背面似乎被撕了张什么标签，残胶还在。\n",
            "分词: ['机', '器', '背', '面', '似', '乎', '被', '撕', '了', '张', '什', '么', '标', '签', '，', '残', '胶', '还', '在', '。']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#编码两个句子\n",
        "sent_encoded = tokenizer.encode(\n",
        "    text=sents[0],\n",
        "    text_pair=sents[1],\n",
        "\n",
        "    #当句子长度大于max_length时,截断\n",
        "    truncation=True,\n",
        "\n",
        "    #一律补pad到max_length长度\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    max_length=30,\n",
        "    return_tensors=None,\n",
        ")\n",
        "\n",
        "print(sent_encoded)\n",
        "\n",
        "tokenizer.decode(sent_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "MrvlptPzYYBN",
        "outputId": "3db4a9d8-4846-4721-a31e-6bdfc7d5711e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#增强的编码函数\n",
        "sent_encoded_plus = tokenizer.encode_plus(\n",
        "    text=sents[0],\n",
        "    text_pair=sents[1],\n",
        "\n",
        "    #当句子长度大于max_length时,截断\n",
        "    truncation=True,\n",
        "\n",
        "    #一律补零到max_length长度\n",
        "    padding='max_length',\n",
        "    max_length=30,\n",
        "    add_special_tokens=True,\n",
        "\n",
        "    #可取值tf,pt,np,默认为返回list\n",
        "    return_tensors=None,\n",
        "\n",
        "    #返回token_type_ids\n",
        "    return_token_type_ids=True,\n",
        "\n",
        "    #返回attention_mask\n",
        "    return_attention_mask=True,\n",
        "\n",
        "    #返回special_tokens_mask 特殊符号标识\n",
        "    return_special_tokens_mask=True,\n",
        "\n",
        "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
        "    #return_offsets_mapping=True,\n",
        "\n",
        "    #返回length 标识长度\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "#input_ids 就是编码后的词\n",
        "#token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
        "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
        "#attention_mask pad的位置是0,其他位置是1\n",
        "#length 返回句子长度\n",
        "for k, v in sent_encoded_plus.items():\n",
        "    print(k, ':', v)\n",
        "\n",
        "tokenizer.decode(sent_encoded_plus['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "V0dJyt_nh-H6",
        "outputId": "cc8c4a66-3ed0-49cb-cba3-2402fcf27450"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : [101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]\n",
            "token_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "special_tokens_mask : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
            "attention_mask : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
            "length : 30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#批量编码句子\n",
        "sent_batch_encoded_plus = tokenizer.batch_encode_plus(\n",
        "    batch_text_or_text_pairs=[sents[0], sents[1]],\n",
        "    add_special_tokens=True,\n",
        "\n",
        "    #当句子长度大于max_length时,截断\n",
        "    truncation=True,\n",
        "\n",
        "    #一律补零到max_length长度\n",
        "    padding='max_length',\n",
        "    max_length=15,\n",
        "\n",
        "    #可取值tf,pt,np,默认为返回list\n",
        "    return_tensors=None,\n",
        "\n",
        "    #返回token_type_ids\n",
        "    return_token_type_ids=True,\n",
        "\n",
        "    #返回attention_mask\n",
        "    return_attention_mask=True,\n",
        "\n",
        "    #返回special_tokens_mask 特殊符号标识\n",
        "    return_special_tokens_mask=True,\n",
        "\n",
        "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
        "    #return_offsets_mapping=True,\n",
        "\n",
        "    #返回length 标识长度\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "#input_ids 就是编码后的词\n",
        "#token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
        "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
        "#attention_mask pad的位置是0,其他位置是1\n",
        "#length 返回句子长度\n",
        "for k, v in sent_batch_encoded_plus.items():\n",
        "    print(k, ':', v)\n",
        "\n",
        "tokenizer.decode(sent_batch_encoded_plus['input_ids'][0]), tokenizer.decode(sent_batch_encoded_plus['input_ids'][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxOpUUV3k11N",
        "outputId": "1f7c386c-396c-4448-9a44-dc661aeaef8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 102], [101, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0]]\n",
            "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]]\n",
            "length : [15, 12]\n",
            "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 [SEP]',\n",
              " '[CLS] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#批量编码成对的句子\n",
        "sent_pair_batch_encoded_plus = tokenizer.batch_encode_plus(\n",
        "    batch_text_or_text_pairs=[(sents[0], sents[1]), (sents[2], sents[3])],\n",
        "    add_special_tokens=True,\n",
        "\n",
        "    #当句子长度大于max_length时,截断\n",
        "    truncation=True,\n",
        "\n",
        "    #一律补零到max_length长度\n",
        "    padding='max_length',\n",
        "    max_length=30,\n",
        "\n",
        "    #可取值tf,pt,np,默认为返回list\n",
        "    return_tensors=None,\n",
        "\n",
        "    #返回token_type_ids\n",
        "    return_token_type_ids=True,\n",
        "\n",
        "    #返回attention_mask\n",
        "    return_attention_mask=True,\n",
        "\n",
        "    #返回special_tokens_mask 特殊符号标识\n",
        "    return_special_tokens_mask=True,\n",
        "\n",
        "    #返回offset_mapping 标识每个词的起止位置,这个参数只能BertTokenizerFast使用\n",
        "    #return_offsets_mapping=True,\n",
        "\n",
        "    #返回length 标识长度\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "#input_ids 就是编码后的词\n",
        "#token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
        "#special_tokens_mask 特殊符号的位置是1,其他位置是0\n",
        "#attention_mask pad的位置是0,其他位置是1\n",
        "#length 返回句子长度\n",
        "for k, v in sent_pair_batch_encoded_plus.items():\n",
        "    print(k, ':', v)\n",
        "\n",
        "tokenizer.decode(sent_pair_batch_encoded_plus['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "IHf7ffPOmDdE",
        "outputId": "dbafd5b1-066b-40b0-b9e1-a3709128d75c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids : [[101, 6848, 2885, 4403, 3736, 5709, 1736, 4638, 1333, 1728, 2218, 3221, 3175, 912, 511, 102, 5011, 6381, 3315, 4638, 7241, 4669, 4802, 2141, 4272, 511, 102, 0, 0, 0], [101, 2791, 7313, 1922, 2207, 511, 1071, 800, 4638, 6963, 671, 5663, 511, 102, 791, 1921, 2798, 4761, 6887, 6821, 741, 6820, 3300, 5018, 127, 1318, 117, 4696, 3300, 102]]\n",
            "token_type_ids : [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "special_tokens_mask : [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
            "length : [27, 30]\n",
            "attention_mask : [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 选 择 珠 江 花 园 的 原 因 就 是 方 便 。 [SEP] 笔 记 本 的 键 盘 确 实 爽 。 [SEP] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#获取字典\n",
        "dictionary = tokenizer.get_vocab()\n",
        "\n",
        "type(dictionary), len(dictionary), '月光' in dictionary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhfgXRYypKDf",
        "outputId": "0cfcef5f-e7f7-4cac-e6ae-3afbefb176b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict, 21128, False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#添加新词\n",
        "tokenizer.add_tokens(new_tokens=['月光', '希望'])\n",
        "\n",
        "#添加新符号\n",
        "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
        "\n",
        "dictionary_new = tokenizer.get_vocab()\n",
        "\n",
        "type(dictionary_new), len(dictionary_new), dictionary_new['月光'], dictionary_new['[EOS]']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAjlNfRuppbN",
        "outputId": "9e9d066b-f662-4734-b549-bf85f3ff5a6c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dict, 21131, 21128, 21130)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#编码新添加的词\n",
        "out = tokenizer.encode(\n",
        "    text='月光的新希望[EOS]',\n",
        "    text_pair=None,\n",
        "\n",
        "    #当句子长度大于max_length时,截断\n",
        "    truncation=True,\n",
        "\n",
        "    #一律补pad到max_length长度\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    max_length=8,\n",
        "    return_tensors=None,\n",
        ")\n",
        "\n",
        "print(out)\n",
        "\n",
        "tokenizer.decode(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4ebW7GQap95E",
        "outputId": "d4bddb57-1448-4a00-c3f8-edffabfc71e2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 21128, 4638, 3173, 21129, 21130, 102, 0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] 月光 的 新 希望 [EOS] [SEP] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}